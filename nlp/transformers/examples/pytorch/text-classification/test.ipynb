{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, AutoTokenizer, \\\n",
    "    AutoModelForSequenceClassification, TrainingArguments, AutoConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google-bert/bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=5,\n",
    "    finetuning_task=\"text-classification\",\n",
    ")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    from_tf=False,\n",
    "    config=config,\n",
    "    trust_remote_code=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset if training_args.do_train else None,\n",
    "        eval_dataset=eval_dataset if training_args.do_eval else None,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tc_mapping = [[-1, 0], [0, 1]]\n",
    "tc_mapping = [[0.5, 1], [0, 0.5], [-1, 0]]\n",
    "preds = [0, 1, 2, 3 , 4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0\n",
      "-0.6\n",
      "-0.19999999999999996\n",
      "0.20000000000000018\n",
      "0.6000000000000001\n"
     ]
    }
   ],
   "source": [
    "new_preds = []\n",
    "for item in preds:\n",
    "    scaled_pred = item * 0.4 - 1\n",
    "    print(scaled_pred)\n",
    "    for i, (low, high) in enumerate(tc_mapping):\n",
    "        if low <= scaled_pred < high:\n",
    "            new_preds.append(i)\n",
    "            break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dataheart/yuhanl/miniconda3/envs/cc/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dataheart/yuhanl/miniconda3/envs/cc/lib/python3.10/site-packages/datasets/load.py:1486: FutureWarning: The repository for cardiffnlp/tweet_topic_multi contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/cardiffnlp/tweet_topic_multi\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Downloading builder script: 100%|██████████| 4.65k/4.65k [00:00<00:00, 2.93MB/s]\n",
      "Downloading readme: 100%|██████████| 9.23k/9.23k [00:00<00:00, 2.90MB/s]\n",
      "Downloading data: 100%|██████████| 201k/201k [00:00<00:00, 4.54MB/s]\n",
      "Downloading data: 100%|██████████| 578k/578k [00:00<00:00, 6.31MB/s]\n",
      "Downloading data: 100%|██████████| 1.62M/1.62M [00:00<00:00, 11.1MB/s]\n",
      "Downloading data: 100%|██████████| 516k/516k [00:00<00:00, 14.3MB/s]\n",
      "Downloading data: 100%|██████████| 203k/203k [00:00<00:00, 4.25MB/s]\n",
      "Downloading data: 100%|██████████| 63.2k/63.2k [00:00<00:00, 15.9MB/s]\n",
      "Downloading data: 100%|██████████| 1.62M/1.62M [00:00<00:00, 21.6MB/s]\n",
      "Downloading data: 100%|██████████| 203k/203k [00:00<00:00, 4.37MB/s]\n",
      "Downloading data: 100%|██████████| 1.96M/1.96M [00:00<00:00, 47.2MB/s]\n",
      "Downloading data: 100%|██████████| 2.03M/2.03M [00:00<00:00, 21.5MB/s]\n",
      "Downloading data: 100%|██████████| 1.96M/1.96M [00:00<00:00, 16.6MB/s]\n",
      "Downloading data: 100%|██████████| 2.03M/2.03M [00:00<00:00, 21.9MB/s]\n",
      "Generating test_2020 split: 573 examples [00:00, 9953.02 examples/s]\n",
      "Generating test_2021 split: 1679 examples [00:00, 12057.26 examples/s]\n",
      "Generating train_2020 split: 4585 examples [00:00, 13037.75 examples/s]\n",
      "Generating train_2021 split: 1505 examples [00:00, 16321.09 examples/s]\n",
      "Generating train_all split: 6090 examples [00:00, 16258.80 examples/s]\n",
      "Generating validation_2020 split: 573 examples [00:00, 14855.95 examples/s]\n",
      "Generating validation_2021 split: 188 examples [00:00, 13208.19 examples/s]\n",
      "Generating train_random split: 4564 examples [00:00, 16226.52 examples/s]\n",
      "Generating validation_random split: 573 examples [00:00, 14850.53 examples/s]\n",
      "Generating test_coling2022_random split: 5536 examples [00:00, 13316.26 examples/s]\n",
      "Generating train_coling2022_random split: 5731 examples [00:00, 15851.65 examples/s]\n",
      "Generating test_coling2022 split: 5536 examples [00:00, 16124.98 examples/s]\n",
      "Generating train_coling2022 split: 5731 examples [00:00, 16326.94 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ds = datasets.load_dataset(\"cardiffnlp/tweet_topic_multi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test_2020: Dataset({\n",
       "        features: ['text', 'date', 'label', 'label_name', 'id'],\n",
       "        num_rows: 573\n",
       "    })\n",
       "    test_2021: Dataset({\n",
       "        features: ['text', 'date', 'label', 'label_name', 'id'],\n",
       "        num_rows: 1679\n",
       "    })\n",
       "    train_2020: Dataset({\n",
       "        features: ['text', 'date', 'label', 'label_name', 'id'],\n",
       "        num_rows: 4585\n",
       "    })\n",
       "    train_2021: Dataset({\n",
       "        features: ['text', 'date', 'label', 'label_name', 'id'],\n",
       "        num_rows: 1505\n",
       "    })\n",
       "    train_all: Dataset({\n",
       "        features: ['text', 'date', 'label', 'label_name', 'id'],\n",
       "        num_rows: 6090\n",
       "    })\n",
       "    validation_2020: Dataset({\n",
       "        features: ['text', 'date', 'label', 'label_name', 'id'],\n",
       "        num_rows: 573\n",
       "    })\n",
       "    validation_2021: Dataset({\n",
       "        features: ['text', 'date', 'label', 'label_name', 'id'],\n",
       "        num_rows: 188\n",
       "    })\n",
       "    train_random: Dataset({\n",
       "        features: ['text', 'date', 'label', 'label_name', 'id'],\n",
       "        num_rows: 4564\n",
       "    })\n",
       "    validation_random: Dataset({\n",
       "        features: ['text', 'date', 'label', 'label_name', 'id'],\n",
       "        num_rows: 573\n",
       "    })\n",
       "    test_coling2022_random: Dataset({\n",
       "        features: ['text', 'date', 'label', 'label_name', 'id'],\n",
       "        num_rows: 5536\n",
       "    })\n",
       "    train_coling2022_random: Dataset({\n",
       "        features: ['text', 'date', 'label', 'label_name', 'id'],\n",
       "        num_rows: 5731\n",
       "    })\n",
       "    test_coling2022: Dataset({\n",
       "        features: ['text', 'date', 'label', 'label_name', 'id'],\n",
       "        num_rows: 5536\n",
       "    })\n",
       "    train_coling2022: Dataset({\n",
       "        features: ['text', 'date', 'label', 'label_name', 'id'],\n",
       "        num_rows: 5731\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
